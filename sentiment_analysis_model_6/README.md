## sentiment-analysis-model

### 🔴 یک اشکال !!
##### 🔺در LR-Scheduler وقتی مانیتور val-loss هست باید mode برابر min باشه !! یعنی کنترل نرخ یادگیری در راستای کاهش val-loss انجام بشه !! در حالی که توی کد اشتباها max گذاشته بودم ! همین موضوع تا حدودی کار خرابی میکرد !! نسخه اصلاح شده را قرار میدهیم
##### 🔺 نوشته های پایین در مورد نتایج آموزش های این کد هستند ( قبل از اینکه بفهمم اشتباها mode  رو max گذاشتم نوشتمشون، ولی پاک نمیکنم تا نسخه اصلی کد با تحلیل نتایجش بمونه !)

---

در این کد چهار مدل Encoder-Only داریم و یک دیتاست 20 کلاسه تحلیل احساسات

models : Estandard-Model , Model-1 , Model-2 , Model-3

#### Estandard-Model : 
یک مدل encoder-only با ترنسفورمر استاندارد

دلیل تعریف این مدل: یک مدل ترنسفورمر استاندارد به عنوان معیار داشته باشیم تا در هر گام عملکرد ۳ مدل را با مدل استاندارد مقایسه کنیم. ( در واقع یک جورایی همواره رفتارشون رو بررسی و کنترل میکنیم.)
#### Model-1 : 
یک مدل encoder-only  با یک ترنسفورمر هیبریدی ساخته شده با کانولوشن یک بعدی ( conv-attention )
#### Model-2 :
یک مدل encoder-only  با یک ترنسفورمر هیبریدی ساخته شده با شبکه عصبی بازگشتی gru ( مکانیزم توجه gru-attention)
#### Model-3 :
یک مدل encoder-only که از هر دو نوع مکانیزم توجه  conv-attention , gru-attention استفاده میکند. این مدل یک سریال از بلاک های انکودر استفاده شده در مدل های بالا است ( به شکل یکی در میون از انکودر های بالا استفاده میکند ). اگر به معماری این مدل توجه کنید میبینید که هر لایه این مدل در واقع دو لایه هستش

میخواهیم عملکرد هر 4 مدل را بر دیتاست بررسی کنیم

#### train-model-2 ( from-scratch )
مدل 2 بر دیتاست اموزش دیده است، ولی خب عملکرد جالبی نشان نداده است، احتمالا اصلی ترین دلیلش هم این است که تعداد ایپاک اموزشی خیلی کم است( مکانیزم توقف زودهنگام باعث توقف فرایند اموزش شد، تنظیماتش باید بهتر شود). این مورد در کد قابل مشاهده است.
دلیل دیگرش احتمالا مکانیزم gru-attention است.این مکانیزم همزمان دو کار انجام میدهد، هم حافظه کوتاه مدت دارد و هم مکانیزم توجه را پیاده میکند.
پس بیشتر با جزییات متن درگیر است و ممکن است این موضوع باعث شده باشد که مدل دیدی کلی به text ها برای طبقه بندی آنها به دست نیاورده باشد.( یا احتمالا به زمان خیلی بیشتری برای به دست اوردن این توانایی نیاز دارد)
به هر حال هنوز از اینکه دلیلش چه بوده که مدل ۲ روی دیتاست خوب عمل نکرده مطمئن نیستیم، برای کسب اطلاعات بیشتر از شیوه عملکرد مدل و تاثیر پارامتر ها رو آن ۲۳ بار روی subset های دیتاست اصلی مدل ها را train کرده ایم. نتایج در جدول زیر قابل مشاهده اند.


### Results
###### نتایج train های انجام شده روی دیتاست های بسیار کوچک در جدول زیر مشخص است. در هر شش Lab هر چهار مدل رو روی دیتاست خیلی کوچکی اموزش داده ایم. در هر اموزش پارامتر ها را تغییر داده ایم تا تاثیر پارامتر های مختلف را بر عملکرد مدل ببینیم. همچنین در این حین میتوان در هر Lab هر چهار مدل را نیز با هم مقایسه کرد.

| - | Lab-num | model | hidden-dim | num-layers | early-stopping (yes/no) | max-epochs | patience | trained-epochs |  min-delta | gradient-clip-val | train-data-size | val-data-size | test-data-size | f1-score | accuracy |
| - | --------| ----- | ---------- | ---------- | ----------------------- | ---------- | -------- | -------------  | ---------- | ---------------- | ---------------- | ------------- | -------------- | ------- | -------- |
| 1 | Lab-1 | estandard-model | 16 | 3 | yes | 6 | 3 | 4 | 0.00 | 1 | 3000 | 300 | 500 | 0.208 | 0.208 |
| 2 | Lab-1 | model-1 | 16 | 3 | yes | 6 | 3 | 4 | 0.00 | 1 | 3000 | 300 | 500 | 0.202 | 0.202 |
| 3 | Lab-1 | model-2 | 16 | 3 | yes | 6 | 3 | 4 | 0.00 | 1 | 3000 | 300 | 500 | 0.202 | 0.202 |
| 4 | Lab-1 | model-3 | 16 | 1 | yes | 6 | 3 | 4 | 0.00 | 1 | 3000| 300 | 500 | 0.202 | 0.202 |
| 5 | Lab-2 | estandard-model | 32 | 2 | yes | 6 | 3 | 4 |  0.00 | 1.0 | 3000 | 300 | 500 | 0.180 | 0.180 |
| 6 | Lab-2 | model-1 | 32 | 2 | yes | 6 | 3 | 6 |  0.00 | 1.0 | 3000 | 300 | 500 | 0.172 | 0.172 |
| 7 | Lab-2 | model-2 | 32 | 2 | yes | 6 | 3 | 4 |  0.00 | 1.0 | 3000 | 300 | 500 | 0.202 | 0.202 |
| 8 | Lab-2 | model-3 | 32 | 1 | yes | 6 | 3 | 4 |  0.00 | 1.0 | 3000 | 300 | 500 | 0.202 | 0.202 |
| 9 | Lab-3 | estandard-model | 16 | 4 | yes | 6 | 3 | 4 |  1e-3 | 1.0 | 3000 | 300 | 500 | 0.164 | 0.164 |
| 10 | Lab-3 | model-1 | 16 | 4 | yes | 6 | 3 | 4 |  1e-3 | 1.0 | 3000 | 300 | 500 | 0.202 | 0.202 |
| 11 | Lab-3 | model-2 | 16 | 4 | yes | 6 | 3 | 4 |  1e-3 | 1.0 | 3000 | 300 | 500 | 0.202 | 0.202 |
| 12 | Lab-3 | model-3 | 16 | 4 | yes | 6 | 3 | 4 |  1e-3 | 1.0 | 3000 | 300 | 500 | 0.202 | 0.202 |
| 13 | Lab-4 | est-model | 64 | 7 | yes | 6 | 3 | 4 |  1e-3 | 1.0 | 5000 | 650 | 800 | 0.207 | 0.207 |
| 14 | Lab-4 | model-1 | 64 | 10 | yes | 6 | 3 | 4 |  1e-3 | 1.0 | 5000 | 650 | 800 | 0.207 | 0.207 |
| 15 | Lab-4 | model-2 | 64 | 7 | yes | 6 | 3 | 4 |  1e-4 | 1.0 | 5000 | 650 | 800 | 0.207 | 0.207 |
| 16 | Lab-4 | model-3 | 64 | 5 | yes | 6 | 3 | 4 |  1e-2 | 1.0 | 5000 | 650 | 800 | 0.207 | 0.207 |
| 17 | Lab-5 | est-model | 8 | 15 | no | 20 | - | 20 |  - | 1.0 | 1000 | 100 | 200 | 0.230 | 0.230 |
| 18 | Lab-5 | model-1 | 8 | 15 | no | 6 | - | 6 |  - | 1.2 | 1000 | 100 | 200 | 0.230 | 0.230 |
| 19 | Lab-5 | model-2 | 8 | 20 | no | 8 | - | 8 |  - | 1.3 | 1000 | 100 | 200 | 0.206 | 0.206 |
| 20 | Lab-5 | model-3 | 8 | 15 | yes | 8 | 6 | 7 | 1e-3 | 1.5 | 1000 | 100 | 200 | 0.206 | 0.206 |
| 21 | Lab-6 | model-1 | 8 | 20 | no | 24 | - | 24 | - | 1.2 | 1000 | 100 | 200 | 0.230 | 0.230 |
| 22 | Lab-6 | model-2 | 4 | 10 | no | 25 | - | 25 | - | 1.3 | 1000 | 100 | 200 | 0.206 | 0.206 |
| 23 | Lab-6 | model-3 | 128 | 10 | no | 15 | - | 15 | - | 1.2 | 3000 | 500 | 600 | 0.200 | 0.200 |

###### پارامتر هایی که در train های مختلف تغییر داده میشدن : gradient-clip-val , min-daita (early-stopping parameter) , patience (early-stopping parameter) , max-epochs , num-layers , hidden-dim

######  hidden-dim : این پارامتر طول برداری رو مشخص میکنه که تکست ورودی قراره بهش تبدیل بشه و به عنوان برداری به این سایز در طول مدل جریان پیدا میکنه
در واقع این پارامتر مثل عرض مدل هستش. هر چه مقدار این پارامتر بیشتر باشه، مدل بهتر جزییات تسکت ها رو یاد میگیره 
###### num-layers : تعداد لایه های مدل (یا تعداد بلاک های انکودر)
###### max-epochs : تعداد ایپاک های آموزشی 
توجه کنید که مدل ما داره from-scartch  اموزش میبینه ( یعنی از صفر). پس بهتره که تعداد ایپاک ها زیاد باشه. مثلا 30 تا 
###### patience : تعداد ایپاک هایی که مکانیزم توقف زودهنگام در صورت بروز شرایط توقف باید صبر کند
###### min-delta : حداقل مقدار تغییری در مقدار متریک ارزیابی که برای مدل پیشرفت محسوب میشود
###### gradient-clip-val : پارامتری هست که گرادیان های آموزش رو کنترل میکنه 
اگه گرادیان ها خیلی زیاد یا کم شدن، مقیاسشون رو تغییر میده. اگه loss خیلی نوسان داشته باشه بهتره کم تر بشه (مثلا 0.5) و اگه گرادیان ها همیشه کوچک میشن و مدل یاد نمیگیره بهتره زیاد بشه ( مثلا 2 یا 5 )

### تفسیر نتایج جدول بالا
همانطور که ملاحظه میکنید تمام f1-score های بالا حول و حوش 20 درصد هستند. فعلا خارج از این محدوده نتونستیم بریم

در کل هر 3 مدل نتایجشون بسیار نزدیک به مدل استاندراد بوده. تفاوت آنچنانی مشاهده نشده فعلا

#### بیشترین دقت و f1-score  برابر 23 درصد شده
این مورد در اجرا های 17 و 18 و 21 مشاهده میشه. پارامتر اجرا های 17 و 18 به جز تعداد ایپاک هاشون و gradient-clip-val مثل همه. یکی با تعداد ایپاک 6 و یکی دیگه با تعداد ایپاک 20 نتیجه مشابهی دادن. پس معلومه این تفاوت تعداد ایپاک های آموزشی زیاد فایده چندانی نداشته. همچنین در اجرای 21 تعداد لایه ها رو 20 کردیم و تعداد ایپاک اموزشی رو 24 تا. در واقع انتظار یک نتیجه خیلی بهتر داشتیم، ولی عملکردش با مدل های 17 و 18 که 15 لایه ای بودن فرق چندانی نداشت. توجه کنید اجرای 18 از اون دوتا بهتره چون همون نتیجه رو با 15 لایه و 6 ایپاک گرفتیم.

#### اجرای 23 ام قابل توجه هست
در اجرای 23 ام اومدم hidden-dim رو 128 گذاشتم ( احتمالا میدونید که 8 و 4 و 16 و.. خیلی کم هستن و انتظار انچنانی هم نمیشه ازشون داشت، صرفا ازمایش میکردیم ). تعداد لایه های این مدل به ظاهر 10 تاست ولی بالا هم گفتیم که مدل 3 هر لایه اش دو تا لایه انکودر داره. یعنی انگار این مدل 20 لایه هستش. از مدلی 20 لایه و hidden-dim برابر 128 با 15 ایپاک آموزشی انتظار نتیجه ای بهتر از نتایج بالا میره ولی همانطور که میبینیم f1-score و دقت 20 درصد شده. یعنی عملا فقط زحمت train زیاد شد ولی نتیجه ای نگرفتیم. مدلی با 15 لایه و hidden-dim برابر 8 و ایپاک 6 در اجرای 18 نتیجه بهتری داشته.

#### اجراهایی که hidden-dim اونها برابر 64 هست ؟!
در بالا قابل مشاهده هستش که اجراهایی که hidden-dim اونها 64 هست، دقتشون 20 درصد شده، تفاوتی با وقتی hidden-dim برابر 8 یا 16 بوده نمیبینیم.
#### دقت یک مدل با افزایش تنها یک لایه از 20 درصد به 16 درصد سقوط کرد !!
مدل استاندارد را در اجراهای 1 و 9 ملاحظه کنید. با افزایش تنها یک لایه و با یکسان بودن بقیه پارامترها f1-score , دقت 4 درصد کم شد. این عجیبه چون با افزایش یک لایه انتظار میرفت حداقل اگه دقت زیاد نمیشه انقدر کم نشه. این **بیشترین تغییر** تو تمام ازمایش های بالا بود.

#
