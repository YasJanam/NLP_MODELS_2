{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qXdL4Q0OYiUm",
    "outputId": "02c84e5f-fe59-4779-8f84-fea3dbbbd9b5"
   },
   "outputs": [],
   "source": [
    "!pip install pytorch_lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "vrq_gmU8X1b7"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.metrics import f1_score\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "4gFd9YZVYbLl"
   },
   "outputs": [],
   "source": [
    "# ===== Mini Dataset =====\n",
    "data = [\n",
    "    (\"Ali lives in Tehran\", [\"PER\",\"O\",\"O\",\"LOC\"]),\n",
    "    (\"Sara works at Google\", [\"PER\",\"O\",\"O\",\"ORG\"]),\n",
    "    (\"I saw John in Paris\", [\"O\",\"O\",\"PER\",\"O\",\"LOC\"])\n",
    "]\n",
    "\n",
    "label2id = {\"O\":0, \"PER\":1, \"LOC\":2, \"ORG\":3}  # توی این کد یک جورایی کار توکنایزر رو میکنه label2id\n",
    "id2label = {0:\"O\", 1:\"PER\", 2:\"LOC\", 3:\"ORG\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "sSAGTY6Pj5Z3"
   },
   "outputs": [],
   "source": [
    "# ===== vocab =====\n",
    "vocab = {\"<PAD>\":0}\n",
    "for sentence, _ in data:\n",
    "  for tok in sentence.split():\n",
    "    if tok not in vocab:\n",
    "      vocab[tok] = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xdbPRJ5czcMT",
    "outputId": "fcaf4d29-3cdd-41dd-d2fb-6736a9d48774"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<PAD>': 0,\n",
       " 'Ali': 1,\n",
       " 'lives': 2,\n",
       " 'in': 3,\n",
       " 'Tehran': 4,\n",
       " 'Sara': 5,\n",
       " 'works': 6,\n",
       " 'at': 7,\n",
       " 'Google': 8,\n",
       " 'I': 9,\n",
       " 'saw': 10,\n",
       " 'John': 11,\n",
       " 'Paris': 12}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "id": "J0nWNXSuZwVf"
   },
   "outputs": [],
   "source": [
    "class MiniNERDataset(Dataset):\n",
    "  def __init__(self, data, vocab, label2id):\n",
    "    self.data = data\n",
    "    self.vocab = vocab\n",
    "    self.label2id = label2id\n",
    "    print(\"MiniNERDataset\")\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    tokens, labels = self.data[idx]\n",
    "    tokens = tokens.split()\n",
    "    input_ids = [self.vocab[tok] for tok in tokens]\n",
    "    label_ids = [self.label2id[l] for l in labels]\n",
    "    return torch.tensor(input_ids), torch.tensor(label_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LFFvqLXRfFg1"
   },
   "source": [
    "**collate_fn**\n",
    "\n",
    "متدی برای پد کردن داده ها برای اینکه همه هم اندازه باشند\n",
    "\n",
    "یک بچ از داده ها را میگرد و همه داده های آن بچ را پد میکند\n",
    "\n",
    "این متد در دیتالودر استفاده میشه\n",
    "\n",
    "در واقع **هر بار که دیتالودر یک بچ برمیگردونه این متد روی اون بچ اعمال میشه**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "id": "AW61SrZLks7i"
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "  print(\"\\n\\n🟨 START 🟨\")\n",
    "  print(\"🍁🍁🍁🍁🍁🍁🍁🍁🍁🍁🍁🍁 collate_fn 🍁🍁🍁🍁🍁🍁🍁🍁🍁🍁🍁🍁\")\n",
    "  input_ids, label_ids = zip(*batch)\n",
    "  print(f\"\\n\\t| input_ids : {input_ids} \")\n",
    "  print(f\"\\t| label_ids : {label_ids} \")\n",
    "  input_ids_padded = pad_sequence(input_ids, batch_first=True, padding_value=0)\n",
    "  label_ids_padded = pad_sequence(label_ids, batch_first=True, padding_value=-100)\n",
    "  print(f\"\\t| pad_input_ids: {input_ids_padded} \")\n",
    "  print(f\"\\t| pad_label_ids: {label_ids_padded} \")\n",
    "  return input_ids_padded, label_ids_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "id": "hZ9oVX8XbK6W"
   },
   "outputs": [],
   "source": [
    "# ===== Simple Encoder =====\n",
    "class SimpleEncoder(nn.Module):\n",
    "  def __init__(self, vocab_size=1000, hidden_dim=32):\n",
    "    super().__init__()\n",
    "    self.embedding = nn.Embedding(vocab_size, hidden_dim)\n",
    "    self.lstm = nn.LSTM(hidden_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "    print(\"Simple Encoder\")\n",
    "\n",
    "  def forward(self, input_ids):\n",
    "    print(\"\\t↪------ SimpleEncoder-class ------\")\n",
    "    print(\"\\t\\t ↪|Forward method :\")\n",
    "    print(\"\\t\\t\\t ↪|lstm کلاس انکودر : یک لایه امبدینگ + یه لایه\")\n",
    "    print(\"\\t\\t\\t ↪|➡➡➡➡➡ embedding ⬅⬅⬅⬅⬅\")\n",
    "    x = self.embedding(input_ids)\n",
    "    print(\"\\t\\t\\t ↪|➡➡➡➡➡ lstm ⬅⬅⬅⬅⬅\")\n",
    "    x,_ = self.lstm(x)\n",
    "    print(\"\\t\\t\\t ↪|OUT-OF-FORWARD (خروج از انکودر)\\n\")\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "id": "VaJvLhizcUiY"
   },
   "outputs": [],
   "source": [
    "# ===== TokenClassifier Model =====\n",
    "class TokenClassifier(nn.Module):\n",
    "  def __init__(self, encoder, num_labels):\n",
    "    super().__init__()\n",
    "    self.encoder = encoder\n",
    "    self.classifier = nn.Linear(encoder.lstm.hidden_size * 2, num_labels)\n",
    "    print(\"TokenClassification\")\n",
    "\n",
    "  def forward(self, input_ids):\n",
    "    print(\"➡➡➡➡➡ TokenClassifier ⬅⬅⬅⬅⬅\")\n",
    "    print(\"\\t ↪|forward method :\")\n",
    "    print(\"\\t\\t ↪|Encoder-Layer\")\n",
    "    print(\"\\t\\t ↪|Linear-Layer\")\n",
    "    print(\"\\n➡➡➡➡➡ Encoder-Layer ⬅⬅⬅⬅⬅\")\n",
    "    x = self.encoder(input_ids)\n",
    "    print(\"➡➡➡➡➡ Linear-Layer ⬅⬅⬅⬅⬅\")\n",
    "    logits = self.classifier(x)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "id": "r0p6TIiwdLi-"
   },
   "outputs": [],
   "source": [
    "# ===== Lightning Module =====\n",
    "class NERModule(pl.LightningModule):\n",
    "  def __init__(self, model, lr=1e-3):\n",
    "    super().__init__()\n",
    "    self.model = model\n",
    "    self.loss_fn = nn.CrossEntropyLoss()\n",
    "    self.lr = lr\n",
    "    self.dataset = MiniNERDataset(data, vocab, label2id) # Store dataset as an attribute\n",
    "    print(\"Lightning Module\")\n",
    "\n",
    "\n",
    "  def train_dataloader(self):\n",
    "    print(\"🟪🟪🟪🟪🟪🟪🟪🟪🟪🟪🟪 train_dataloader method 🟪🟪🟪🟪🟪🟪🟪🟪🟪🟪🟪\\n\")\n",
    "    dataloader = DataLoader(self.dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "    #print(\"\\n⭐ 2-BATCHES ⭐\\n\")\n",
    "    return dataloader # Use collate_fn\n",
    "\n",
    "  def training_step(self, batch, batch_idx):\n",
    "    print(\"\\n◼◼◼◼◼◼◼◼◼◼◼◼  training_step  ◼◼◼◼◼◼◼◼◼◼◼◼\")\n",
    "    input_ids, labels = batch\n",
    "    print(\"\\n◻◻◻◻◻◻◻◻◻◻◻◻ call model ◻◻◻◻◻◻◻◻◻◻◻◻\")\n",
    "    logits = self.model(input_ids)\n",
    "    print(\"\\n◻◻◻◻◻◻◻◻◻◻◻◻ calculate loss ◻◻◻◻◻◻◻◻◻◻◻◻\")\n",
    "    loss = self.loss_fn(logits.view(-1, logits.shape[-1]), labels.view(-1))\n",
    "    print(f\"loss : {loss}\")\n",
    "    self.log(\"train_loss\", loss)\n",
    "    return loss\n",
    "\n",
    "\n",
    "  def configure_optimizers(self):\n",
    "    print(\"\\n\\n\\n 🟩🟩🟩🟩🟩🟩🟩🟩 configure_optimizers method 🟩🟩🟩🟩🟩🟩🟩🟩\")\n",
    "    print(f\"optimizer : {torch.optim.Adam(self.parameters(), lr=self.lr)}\\n\\n\")\n",
    "    return torch.optim.Adam(self.parameters(), lr=self.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "2381f0cd21a540bf81df0f94b36cfa72",
      "b5dae423a50041bf9ab946ad6dafb672",
      "6ee44721936441c4a289531ae43d83a9",
      "520eea220c2a42f0999d7a0bf0f0ce0e",
      "6a2617ebf6f34ca192947eee2053f27c",
      "f0dbc73d30fe4a919b80c94668399c1c",
      "fe7afee596c74b2bb23ef4416f0f9db5",
      "e67979f7cd314306b2df3cd52cd18ebc",
      "9654698b588b43e9bc77a142fd3c323f",
      "bef8a42bff5748e894d8cedde17e1633",
      "a088cda61cc740799d6cb143fe7d6ffc"
     ]
    },
    "id": "z_LTg9DEfqLQ",
    "outputId": "e072424b-e8e4-4649-82b1-2d5290163e60"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.rank_zero:💡 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO:pytorch_lightning.callbacks.model_summary:\n",
      "  | Name    | Type             | Params | Mode \n",
      "-----------------------------------------------------\n",
      "0 | model   | TokenClassifier  | 17.6 K | train\n",
      "1 | loss_fn | CrossEntropyLoss | 0      | train\n",
      "-----------------------------------------------------\n",
      "17.6 K    Trainable params\n",
      "0         Non-trainable params\n",
      "17.6 K    Total params\n",
      "0.070     Total estimated model params size (MB)\n",
      "6         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-- Encoder -->\n",
      "Simple Encoder\n",
      "\n",
      "-- Model -->\n",
      "TokenClassification\n",
      "\n",
      "-- NERModule -->\n",
      "MiniNERDataset\n",
      "Lightning Module\n",
      "\n",
      "-- START Training -->\n",
      "\n",
      "\n",
      "\n",
      " 🟩🟩🟩🟩🟩🟩🟩🟩 configure_optimizers method 🟩🟩🟩🟩🟩🟩🟩🟩\n",
      "optimizer : Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    decoupled_weight_decay: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "\n",
      "\n",
      "🟪🟪🟪🟪🟪🟪🟪🟪🟪🟪🟪 train_dataloader method 🟪🟪🟪🟪🟪🟪🟪🟪🟪🟪🟪\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2381f0cd21a540bf81df0f94b36cfa72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "🟨 START 🟨\n",
      "🍁🍁🍁🍁🍁🍁🍁🍁🍁🍁🍁🍁 collate_fn 🍁🍁🍁🍁🍁🍁🍁🍁🍁🍁🍁🍁\n",
      "\n",
      "\t| input_ids : (tensor([ 9, 10, 11,  3, 12]), tensor([5, 6, 7, 8])) \n",
      "\t| label_ids : (tensor([0, 0, 1, 0, 2]), tensor([1, 0, 0, 3])) \n",
      "\t| pad_input_ids: tensor([[ 9, 10, 11,  3, 12],\n",
      "        [ 5,  6,  7,  8,  0]]) \n",
      "\t| pad_label_ids: tensor([[   0,    0,    1,    0,    2],\n",
      "        [   1,    0,    0,    3, -100]]) \n",
      "\n",
      "◼◼◼◼◼◼◼◼◼◼◼◼  training_step  ◼◼◼◼◼◼◼◼◼◼◼◼\n",
      "\n",
      "◻◻◻◻◻◻◻◻◻◻◻◻ call model ◻◻◻◻◻◻◻◻◻◻◻◻\n",
      "➡➡➡➡➡ TokenClassifier ⬅⬅⬅⬅⬅\n",
      "\t ↪|forward method :\n",
      "\t\t ↪|Encoder-Layer\n",
      "\t\t ↪|Linear-Layer\n",
      "\n",
      "➡➡➡➡➡ Encoder-Layer ⬅⬅⬅⬅⬅\n",
      "\t↪------ SimpleEncoder-class ------\n",
      "\t\t ↪|Forward method :\n",
      "\t\t\t ↪|lstm کلاس انکودر : یک لایه امبدینگ + یه لایه\n",
      "\t\t\t ↪|➡➡➡➡➡ embedding ⬅⬅⬅⬅⬅\n",
      "\t\t\t ↪|➡➡➡➡➡ lstm ⬅⬅⬅⬅⬅\n",
      "\t\t\t ↪|OUT-OF-FORWARD (خروج از انکودر)\n",
      "\n",
      "➡➡➡➡➡ Linear-Layer ⬅⬅⬅⬅⬅\n",
      "\n",
      "◻◻◻◻◻◻◻◻◻◻◻◻ calculate loss ◻◻◻◻◻◻◻◻◻◻◻◻\n",
      "loss : 1.3405110836029053\n",
      "\n",
      "\n",
      "🟨 START 🟨\n",
      "🍁🍁🍁🍁🍁🍁🍁🍁🍁🍁🍁🍁 collate_fn 🍁🍁🍁🍁🍁🍁🍁🍁🍁🍁🍁🍁\n",
      "\n",
      "\t| input_ids : (tensor([1, 2, 3, 4]),) \n",
      "\t| label_ids : (tensor([1, 0, 0, 2]),) \n",
      "\t| pad_input_ids: tensor([[1, 2, 3, 4]]) \n",
      "\t| pad_label_ids: tensor([[1, 0, 0, 2]]) \n",
      "\n",
      "◼◼◼◼◼◼◼◼◼◼◼◼  training_step  ◼◼◼◼◼◼◼◼◼◼◼◼\n",
      "\n",
      "◻◻◻◻◻◻◻◻◻◻◻◻ call model ◻◻◻◻◻◻◻◻◻◻◻◻\n",
      "➡➡➡➡➡ TokenClassifier ⬅⬅⬅⬅⬅\n",
      "\t ↪|forward method :\n",
      "\t\t ↪|Encoder-Layer\n",
      "\t\t ↪|Linear-Layer\n",
      "\n",
      "➡➡➡➡➡ Encoder-Layer ⬅⬅⬅⬅⬅\n",
      "\t↪------ SimpleEncoder-class ------\n",
      "\t\t ↪|Forward method :\n",
      "\t\t\t ↪|lstm کلاس انکودر : یک لایه امبدینگ + یه لایه\n",
      "\t\t\t ↪|➡➡➡➡➡ embedding ⬅⬅⬅⬅⬅\n",
      "\t\t\t ↪|➡➡➡➡➡ lstm ⬅⬅⬅⬅⬅\n",
      "\t\t\t ↪|OUT-OF-FORWARD (خروج از انکودر)\n",
      "\n",
      "➡➡➡➡➡ Linear-Layer ⬅⬅⬅⬅⬅\n",
      "\n",
      "◻◻◻◻◻◻◻◻◻◻◻◻ calculate loss ◻◻◻◻◻◻◻◻◻◻◻◻\n",
      "loss : 1.427093267440796\n",
      "\n",
      "\n",
      "🟨 START 🟨\n",
      "🍁🍁🍁🍁🍁🍁🍁🍁🍁🍁🍁🍁 collate_fn 🍁🍁🍁🍁🍁🍁🍁🍁🍁🍁🍁🍁\n",
      "\n",
      "\t| input_ids : (tensor([1, 2, 3, 4]), tensor([ 9, 10, 11,  3, 12])) \n",
      "\t| label_ids : (tensor([1, 0, 0, 2]), tensor([0, 0, 1, 0, 2])) \n",
      "\t| pad_input_ids: tensor([[ 1,  2,  3,  4,  0],\n",
      "        [ 9, 10, 11,  3, 12]]) \n",
      "\t| pad_label_ids: tensor([[   1,    0,    0,    2, -100],\n",
      "        [   0,    0,    1,    0,    2]]) \n",
      "\n",
      "◼◼◼◼◼◼◼◼◼◼◼◼  training_step  ◼◼◼◼◼◼◼◼◼◼◼◼\n",
      "\n",
      "◻◻◻◻◻◻◻◻◻◻◻◻ call model ◻◻◻◻◻◻◻◻◻◻◻◻\n",
      "➡➡➡➡➡ TokenClassifier ⬅⬅⬅⬅⬅\n",
      "\t ↪|forward method :\n",
      "\t\t ↪|Encoder-Layer\n",
      "\t\t ↪|Linear-Layer\n",
      "\n",
      "➡➡➡➡➡ Encoder-Layer ⬅⬅⬅⬅⬅\n",
      "\t↪------ SimpleEncoder-class ------\n",
      "\t\t ↪|Forward method :\n",
      "\t\t\t ↪|lstm کلاس انکودر : یک لایه امبدینگ + یه لایه\n",
      "\t\t\t ↪|➡➡➡➡➡ embedding ⬅⬅⬅⬅⬅\n",
      "\t\t\t ↪|➡➡➡➡➡ lstm ⬅⬅⬅⬅⬅\n",
      "\t\t\t ↪|OUT-OF-FORWARD (خروج از انکودر)\n",
      "\n",
      "➡➡➡➡➡ Linear-Layer ⬅⬅⬅⬅⬅\n",
      "\n",
      "◻◻◻◻◻◻◻◻◻◻◻◻ calculate loss ◻◻◻◻◻◻◻◻◻◻◻◻\n",
      "loss : 1.3595505952835083\n",
      "\n",
      "\n",
      "🟨 START 🟨\n",
      "🍁🍁🍁🍁🍁🍁🍁🍁🍁🍁🍁🍁 collate_fn 🍁🍁🍁🍁🍁🍁🍁🍁🍁🍁🍁🍁\n",
      "\n",
      "\t| input_ids : (tensor([5, 6, 7, 8]),) \n",
      "\t| label_ids : (tensor([1, 0, 0, 3]),) \n",
      "\t| pad_input_ids: tensor([[5, 6, 7, 8]]) \n",
      "\t| pad_label_ids: tensor([[1, 0, 0, 3]]) \n",
      "\n",
      "◼◼◼◼◼◼◼◼◼◼◼◼  training_step  ◼◼◼◼◼◼◼◼◼◼◼◼\n",
      "\n",
      "◻◻◻◻◻◻◻◻◻◻◻◻ call model ◻◻◻◻◻◻◻◻◻◻◻◻\n",
      "➡➡➡➡➡ TokenClassifier ⬅⬅⬅⬅⬅\n",
      "\t ↪|forward method :\n",
      "\t\t ↪|Encoder-Layer\n",
      "\t\t ↪|Linear-Layer\n",
      "\n",
      "➡➡➡➡➡ Encoder-Layer ⬅⬅⬅⬅⬅\n",
      "\t↪------ SimpleEncoder-class ------\n",
      "\t\t ↪|Forward method :\n",
      "\t\t\t ↪|lstm کلاس انکودر : یک لایه امبدینگ + یه لایه\n",
      "\t\t\t ↪|➡➡➡➡➡ embedding ⬅⬅⬅⬅⬅\n",
      "\t\t\t ↪|➡➡➡➡➡ lstm ⬅⬅⬅⬅⬅\n",
      "\t\t\t ↪|OUT-OF-FORWARD (خروج از انکودر)\n",
      "\n",
      "➡➡➡➡➡ Linear-Layer ⬅⬅⬅⬅⬅\n",
      "\n",
      "◻◻◻◻◻◻◻◻◻◻◻◻ calculate loss ◻◻◻◻◻◻◻◻◻◻◻◻\n",
      "loss : 1.2867932319641113\n",
      "\n",
      "\n",
      "🟨 START 🟨\n",
      "🍁🍁🍁🍁🍁🍁🍁🍁🍁🍁🍁🍁 collate_fn 🍁🍁🍁🍁🍁🍁🍁🍁🍁🍁🍁🍁\n",
      "\n",
      "\t| input_ids : (tensor([ 9, 10, 11,  3, 12]), tensor([5, 6, 7, 8])) \n",
      "\t| label_ids : (tensor([0, 0, 1, 0, 2]), tensor([1, 0, 0, 3])) \n",
      "\t| pad_input_ids: tensor([[ 9, 10, 11,  3, 12],\n",
      "        [ 5,  6,  7,  8,  0]]) \n",
      "\t| pad_label_ids: tensor([[   0,    0,    1,    0,    2],\n",
      "        [   1,    0,    0,    3, -100]]) \n",
      "\n",
      "◼◼◼◼◼◼◼◼◼◼◼◼  training_step  ◼◼◼◼◼◼◼◼◼◼◼◼\n",
      "\n",
      "◻◻◻◻◻◻◻◻◻◻◻◻ call model ◻◻◻◻◻◻◻◻◻◻◻◻\n",
      "➡➡➡➡➡ TokenClassifier ⬅⬅⬅⬅⬅\n",
      "\t ↪|forward method :\n",
      "\t\t ↪|Encoder-Layer\n",
      "\t\t ↪|Linear-Layer\n",
      "\n",
      "➡➡➡➡➡ Encoder-Layer ⬅⬅⬅⬅⬅\n",
      "\t↪------ SimpleEncoder-class ------\n",
      "\t\t ↪|Forward method :\n",
      "\t\t\t ↪|lstm کلاس انکودر : یک لایه امبدینگ + یه لایه\n",
      "\t\t\t ↪|➡➡➡➡➡ embedding ⬅⬅⬅⬅⬅\n",
      "\t\t\t ↪|➡➡➡➡➡ lstm ⬅⬅⬅⬅⬅\n",
      "\t\t\t ↪|OUT-OF-FORWARD (خروج از انکودر)\n",
      "\n",
      "➡➡➡➡➡ Linear-Layer ⬅⬅⬅⬅⬅\n",
      "\n",
      "◻◻◻◻◻◻◻◻◻◻◻◻ calculate loss ◻◻◻◻◻◻◻◻◻◻◻◻\n",
      "loss : 1.2841746807098389\n",
      "\n",
      "\n",
      "🟨 START 🟨\n",
      "🍁🍁🍁🍁🍁🍁🍁🍁🍁🍁🍁🍁 collate_fn 🍁🍁🍁🍁🍁🍁🍁🍁🍁🍁🍁🍁\n",
      "\n",
      "\t| input_ids : (tensor([1, 2, 3, 4]),) \n",
      "\t| label_ids : (tensor([1, 0, 0, 2]),) \n",
      "\t| pad_input_ids: tensor([[1, 2, 3, 4]]) \n",
      "\t| pad_label_ids: tensor([[1, 0, 0, 2]]) \n",
      "\n",
      "◼◼◼◼◼◼◼◼◼◼◼◼  training_step  ◼◼◼◼◼◼◼◼◼◼◼◼\n",
      "\n",
      "◻◻◻◻◻◻◻◻◻◻◻◻ call model ◻◻◻◻◻◻◻◻◻◻◻◻\n",
      "➡➡➡➡➡ TokenClassifier ⬅⬅⬅⬅⬅\n",
      "\t ↪|forward method :\n",
      "\t\t ↪|Encoder-Layer\n",
      "\t\t ↪|Linear-Layer\n",
      "\n",
      "➡➡➡➡➡ Encoder-Layer ⬅⬅⬅⬅⬅\n",
      "\t↪------ SimpleEncoder-class ------\n",
      "\t\t ↪|Forward method :\n",
      "\t\t\t ↪|lstm کلاس انکودر : یک لایه امبدینگ + یه لایه\n",
      "\t\t\t ↪|➡➡➡➡➡ embedding ⬅⬅⬅⬅⬅\n",
      "\t\t\t ↪|➡➡➡➡➡ lstm ⬅⬅⬅⬅⬅\n",
      "\t\t\t ↪|OUT-OF-FORWARD (خروج از انکودر)\n",
      "\n",
      "➡➡➡➡➡ Linear-Layer ⬅⬅⬅⬅⬅\n",
      "\n",
      "◻◻◻◻◻◻◻◻◻◻◻◻ calculate loss ◻◻◻◻◻◻◻◻◻◻◻◻\n",
      "loss : 1.362076759338379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=3` reached.\n"
     ]
    }
   ],
   "source": [
    "# ===== Training =====\n",
    "print(\"\\n-- Encoder -->\")\n",
    "encoder = SimpleEncoder(vocab_size=len(vocab))\n",
    "print(\"\\n-- Model -->\")\n",
    "model = TokenClassifier(encoder, num_labels=len(label2id))\n",
    "print(\"\\n-- NERModule -->\")\n",
    "ner_module = NERModule(model)\n",
    "\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=3, accelerator='cpu',logger=False)\n",
    "print(\"\\n-- START Training -->\")\n",
    "trainer.fit(ner_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zxskQF19BxQe"
   },
   "source": [
    "---\n",
    "\n",
    "شش بار فراخوانی شده training_step  اگر توجه کنید در بالا متد\n",
    "\n",
    "به این دلیل که ایپاک برابر 3 است و بچ برابر 2\n",
    "\n",
    "epoch = 3 , batch-size = 2 ----> 6 × call training_step\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "cells": [],
  "kernelspec": {
   "display_name": "",
   "name": ""
  },
  "language_info": {
   "name": ""
  },
  "metadata": {
   "kernelspec": {
    "display_name": "Python 3",
    "language": "python",
    "name": "python3"
   },
   "language_info": {
    "name": "python",
    "version": "3.10.12"
   }
  },
  "nbformat": 4,
  "nbformat_minor": 5
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
