{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPmpAHbpgiOmCo6KJnKsvoj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YasJanam/NLP_MODELS_2/blob/main/TinyNERModel_1/TinyNERModel_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch_lightning"
      ],
      "metadata": {
        "collapsed": true,
        "id": "qXdL4Q0OYiUm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vrq_gmU8X1b7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pytorch_lightning as pl\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from sklearn.metrics import f1_score\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Mini Dataset =====\n",
        "data = [\n",
        "    (\"Ali lives in Tehran\", [\"PER\",\"O\",\"O\",\"LOC\"]),\n",
        "    (\"Sara works at Google\", [\"PER\",\"O\",\"O\",\"ORG\"]),\n",
        "    (\"I saw John in Paris\", [\"O\",\"O\",\"PER\",\"O\",\"LOC\"])\n",
        "]\n",
        "\n",
        "label2id = {\"O\":0, \"PER\":1, \"LOC\":2, \"ORG\":3}\n",
        "id2label = {0:\"O\", 1:\"PER\", 2:\"LOC\", 3:\"ORG\"}"
      ],
      "metadata": {
        "id": "4gFd9YZVYbLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== vocab =====\n",
        "vocab = {\"<PAD>\":0}\n",
        "for sentence, _ in data:\n",
        "  for tok in sentence.split():\n",
        "    if tok not in vocab:\n",
        "      vocab[tok] = len(vocab)"
      ],
      "metadata": {
        "id": "sSAGTY6Pj5Z3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MiniNERDataset(Dataset):\n",
        "  def __init__(self, data, vocab, label2id):\n",
        "    self.data = data\n",
        "    self.vocab = vocab\n",
        "    self.label2id = label2id\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    tokens, labels = self.data[idx]\n",
        "    tokens = tokens.split()\n",
        "    input_ids = [self.vocab[tok] for tok in tokens]\n",
        "    label_ids = [self.label2id[l] for l in labels]\n",
        "    return torch.tensor(input_ids), torch.tensor(label_ids)"
      ],
      "metadata": {
        "id": "J0nWNXSuZwVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**collate_fn**\n",
        "\n",
        "متدی برای پد کردن داده ها برای اینکه همه هم اندازه باشند\n",
        "\n",
        "یک بچ از داده ها را میگرد و همه داده های آن بچ را پد میکند\n",
        "\n",
        "این متد در دیتالودر استفاده میشه\n",
        "\n",
        "در واقع **هر بار که دیتالودر یک بچ برمیگردونه این متد روی اون بچ اعمال میشه**"
      ],
      "metadata": {
        "id": "968RT7n_f5hS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "  input_ids, label_ids = zip(*batch)\n",
        "  input_ids_padded = pad_sequence(input_ids, batch_first=True, padding_value=0)\n",
        "  label_ids_padded = pad_sequence(label_ids, batch_first=True, padding_value=-100)\n",
        "  return input_ids_padded, label_ids_padded"
      ],
      "metadata": {
        "id": "AW61SrZLks7i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Simple Encoder =====\n",
        "class SimpleEncoder(nn.Module):\n",
        "  def __init__(self, vocab_size=1000, hidden_dim=32):\n",
        "    super().__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size, hidden_dim)\n",
        "    self.lstm = nn.LSTM(hidden_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "\n",
        "  def forward(self, input_ids):\n",
        "    x = self.embedding(input_ids)\n",
        "    x,_ = self.lstm(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "hZ9oVX8XbK6W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== TokenClassifier Model =====\n",
        "class TokenClassifier(nn.Module):\n",
        "  def __init__(self, encoder, num_labels):\n",
        "    super().__init__()\n",
        "    self.encoder = encoder\n",
        "    self.classifier = nn.Linear(encoder.lstm.hidden_size * 2, num_labels)\n",
        "\n",
        "  def forward(self, input_ids):\n",
        "    x = self.encoder(input_ids)\n",
        "    logits = self.classifier(x)\n",
        "    return logits"
      ],
      "metadata": {
        "id": "VaJvLhizcUiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Lightning Module =====\n",
        "class NERModule(pl.LightningModule):\n",
        "  def __init__(self, model, lr=1e-3):\n",
        "    super().__init__()\n",
        "    self.model = model\n",
        "    self.loss_fn = nn.CrossEntropyLoss()\n",
        "    self.lr = lr\n",
        "    self.dataset = MiniNERDataset(data, vocab, label2id) # Store dataset as an attribute\n",
        "\n",
        "\n",
        "  def train_dataloader(self):\n",
        "    return DataLoader(self.dataset, batch_size=2, shuffle=True, collate_fn=collate_fn) # Use collate_fn\n",
        "\n",
        "  def training_step(self, batch, batch_idx):\n",
        "    # Unpack the tuple provided by the DataLoader with collate_fn\n",
        "    input_ids, labels = batch\n",
        "\n",
        "    logits = self.model(input_ids)\n",
        "    loss = self.loss_fn(logits.view(-1, logits.shape[-1]), labels.view(-1))\n",
        "    self.log(\"train_loss\", loss)\n",
        "    return loss\n",
        "\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "    return torch.optim.Adam(self.parameters(), lr=self.lr)"
      ],
      "metadata": {
        "id": "r0p6TIiwdLi-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dataset = MiniNERDataset(data, vocab, label2id)\n",
        "#loader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "5XR__TdSqpZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Training =====\n",
        "encoder = SimpleEncoder(vocab_size=len(vocab))\n",
        "model = TokenClassifier(encoder, num_labels=len(label2id))\n",
        "ner_module = NERModule(model)\n",
        "\n",
        "trainer = pl.Trainer(max_epochs=3, accelerator='cpu',logger=False)\n",
        "trainer.fit(ner_module)"
      ],
      "metadata": {
        "id": "z_LTg9DEfqLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ner_module)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGtGfr0JsQGa",
        "outputId": "1957e79e-db47-4456-c837-b4ea122e8328"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NERModule(\n",
            "  (model): TokenClassifier(\n",
            "    (encoder): SimpleEncoder(\n",
            "      (embedding): Embedding(13, 32)\n",
            "      (lstm): LSTM(32, 32, batch_first=True, bidirectional=True)\n",
            "    )\n",
            "    (classifier): Linear(in_features=64, out_features=4, bias=True)\n",
            "  )\n",
            "  (loss_fn): CrossEntropyLoss()\n",
            ")\n"
          ]
        }
      ]
    }
  ]
}